# Sign Language Action Detection

## Overview

This project aims to facilitate the understanding and translation of sign language through action detection. It uses machine learning models to identify and classify sign language gestures from video input. The repository contains two Jupyter notebooks that demonstrate the action detection process: `Action Detection Refined.ipynb` and `Action Detection Tutorial.ipynb`.

## Features

- Real-time sign language action detection
- Detailed tutorial on setting up and running the detection model
- Refined model for improved accuracy and performance

## Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.

### Prerequisites

Before running this project, you need to install the following software:

- Python 3.8 or later
- Jupyter Notebook
- Necessary Python libraries: numpy, opencv-python, tensorflow, etc.

### Installation

1. Clone the repository to your local machine:
   ```
   git clone https://github.com/your-username/sign-language-action-detection.git
   ```
2. Navigate to the project directory:
   ```
   cd sign-language-action-detection
   ```
3. Install the required Python packages:
   ```
   pip install -r requirements.txt
   ```

### Running the Notebooks

- Open Jupyter Notebook in the project directory:
  ```
  jupyter notebook
  ```
- Navigate to `Action Detection Tutorial.ipynb` for a step-by-step guide on how the action detection works.
- Explore `Action Detection Refined.ipynb` for an advanced and optimized version of the action detection model.
